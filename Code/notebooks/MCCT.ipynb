{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeRrAoEm8xg"
      },
      "source": [
        "# Imports and Drive Acces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN3I-q84nCE2",
        "outputId": "cc46d83c-4935-424b-99d5-dd41ce1d32da"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vmCf8ql2Flz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import v2, Lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOkFS6unFCa"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWk6InqPnKec"
      },
      "outputs": [],
      "source": [
        "# Extract the image tar files\n",
        "!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/train_images.tar\" -C \"/content/\"\n",
        "!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/val_images.tar\" -C \"/content/\"\n",
        "!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/test_images.tar\" -C \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoWreHZpnMoF"
      },
      "outputs": [],
      "source": [
        "# Define train data path\n",
        "train_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/train_annotations.csv\"\n",
        "train_images = \"/content/train_images\"\n",
        "\n",
        "# Define test data path\n",
        "val_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/val_annotations.csv\"\n",
        "val_images = \"/content/val_images\"\n",
        "\n",
        "# Define test data path\n",
        "test_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/test_annotations.csv\"\n",
        "test_images = \"/content/test_images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dAWL-V6nOo9"
      },
      "outputs": [],
      "source": [
        "class AffectNet(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset subclass for handling the AffectNet dataset.\n",
        "\n",
        "    Attributes:\n",
        "        annotations (DataFrame): The annotations for the images.\n",
        "        root_dir (str): The root directory where the images are stored.\n",
        "        transform (callable, optional): Optional transform to be applied on an image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, annotations_file, img_root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Initializes the AffectNet dataset.\n",
        "\n",
        "        Args:\n",
        "            annotations_file (str): The path to the CSV file containing the annotations.\n",
        "            img_root_dir (str): The root directory where the images are stored.\n",
        "            transform (callable, optional): Optional transform to be applied on an image.\n",
        "        \"\"\"\n",
        "\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.root_dir = img_root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Check if number of images and annotations match\n",
        "        if len(self.annotations) != len(os.listdir(self.root_dir)):\n",
        "            raise ValueError(f\"Number of images and annotations do not match:\\\n",
        "            {len(self.annotations)} != {len(os.listdir(self.root_dir))}\"\n",
        "                             )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The length of the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.annotations)\n",
        "\n",
        "\n",
        "    def sample_dist(self):\n",
        "        val_count = self.annotations.expression.value_counts()\n",
        "        val_count = val_count.to_dict()\n",
        "        category_weights = [1 / val_count[i] for i in sorted(val_count.keys())]\n",
        "        return category_weights\n",
        "\n",
        "\n",
        "    def sample_weights(self):\n",
        "        category_weights = self.sample_dist()\n",
        "        sample_weights = [category_weights[exp] for exp in self.annotations.expression.values]\n",
        "        return sample_weights\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the image and its labels at the given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the image.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the image, and its labels.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get image name and create path\n",
        "        img_name = f\"{self.annotations.iloc[idx, 0]}.jpg\"\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "\n",
        "        # Read image\n",
        "        image = read_image(img_path)\n",
        "\n",
        "        # Get labels and convert to tensor\n",
        "        labels = self.annotations.iloc[idx, -1]\n",
        "        labels = torch.tensor(labels)\n",
        "        # Apply input transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Return image and labels\n",
        "        return image, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gx0SpXXiXaR"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7pJP0hZV3Px"
      },
      "source": [
        "## MFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ5GrAlHV21Y"
      },
      "outputs": [],
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "def l2_norm(input,axis=1):\n",
        "    norm = torch.norm(input,2,axis,True)\n",
        "    output = torch.div(input, norm)\n",
        "    return output\n",
        "\n",
        "class Conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Conv_block, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.prelu = nn.PReLU(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n",
        "\n",
        "class Linear_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Linear_block, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class Depth_Wise(nn.Module):\n",
        "    def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
        "        super(Depth_Wise, self).__init__()\n",
        "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
        "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.residual = residual\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            short_cut = x\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_dw(x)\n",
        "        x = self.project(x)\n",
        "        if self.residual:\n",
        "            output = short_cut + x\n",
        "        else:\n",
        "            output = x\n",
        "        return output\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Swish, self).__init__()\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "NON_LINEARITY = {\n",
        "    'ReLU': nn.ReLU(inplace=True),\n",
        "    'Swish': Swish(),\n",
        "}\n",
        "\n",
        "\n",
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "class swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class CoordAtt(nn.Module):\n",
        "    def __init__(self, inp, oup, groups=32):\n",
        "        super(CoordAtt, self).__init__()\n",
        "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
        "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "        mip = max(8, inp // groups)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(mip)\n",
        "        self.conv2 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv3 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
        "        self.relu = h_swish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        n,c,h,w = x.size()\n",
        "        x_h = self.pool_h(x)\n",
        "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
        "\n",
        "        y = torch.cat([x_h, x_w], dim=2)\n",
        "        y = self.conv1(y)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu(y)\n",
        "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
        "        x_w = x_w.permute(0, 1, 3, 2)\n",
        "\n",
        "        x_h = self.conv2(x_h).sigmoid()\n",
        "        x_w = self.conv3(x_w).sigmoid()\n",
        "        x_h = x_h.expand(-1, -1, h, w)\n",
        "        x_w = x_w.expand(-1, -1, h, w)\n",
        "\n",
        "        y = identity * x_w * x_h\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class MDConv(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, split_out_channels, stride):\n",
        "        super(MDConv, self).__init__()\n",
        "        self.num_groups = len(kernel_size)\n",
        "        self.split_channels = split_out_channels\n",
        "        self.mixed_depthwise_conv = nn.ModuleList()\n",
        "        for i in range(self.num_groups):\n",
        "            self.mixed_depthwise_conv.append(nn.Conv2d(\n",
        "                self.split_channels[i],\n",
        "                self.split_channels[i],\n",
        "                kernel_size[i],\n",
        "                stride=stride,\n",
        "                padding=kernel_size[i]//2,\n",
        "                groups=self.split_channels[i],\n",
        "                bias=False\n",
        "            ))\n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "        self.prelu = nn.PReLU(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.num_groups == 1:\n",
        "            return self.mixed_depthwise_conv[0](x)\n",
        "\n",
        "        x_split = torch.split(x, self.split_channels, dim=1)\n",
        "        x = [conv(t) for conv, t in zip(self.mixed_depthwise_conv, x_split)]\n",
        "        x = torch.cat(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Mix_Depth_Wise(nn.Module):\n",
        "    def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, kernel_size=[3,5,7], split_out_channels=[64,32,32]):\n",
        "        super(Mix_Depth_Wise, self).__init__()\n",
        "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.conv_dw = MDConv(channels=groups, kernel_size=kernel_size, split_out_channels=split_out_channels, stride=stride)\n",
        "        self.CA = CoordAtt(groups, groups)\n",
        "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.residual = residual\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            short_cut = x\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_dw(x)\n",
        "        x = self.CA(x)\n",
        "        x = self.project(x)\n",
        "        if self.residual:\n",
        "            output = short_cut + x\n",
        "        else:\n",
        "            output = x\n",
        "        return output\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
        "        super(Residual, self).__init__()\n",
        "        modules = []\n",
        "        for _ in range(num_block):\n",
        "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
        "        self.model = nn.Sequential(*modules)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Mix_Residual(nn.Module):\n",
        "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[64,64]):\n",
        "        super(Mix_Residual, self).__init__()\n",
        "        modules = []\n",
        "        for _ in range(num_block):\n",
        "            modules.append(Mix_Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups, kernel_size=kernel_size, split_out_channels=split_out_channels ))\n",
        "        self.model = nn.Sequential(*modules)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MixedFeatureNet(nn.Module):\n",
        "    def __init__(self, embedding_size=256, out_h=7, out_w=7):\n",
        "        super(MixedFeatureNet, self).__init__()\n",
        "        #224x224\n",
        "        self.conv0 = Conv_block(3, 3, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        #112x112\n",
        "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        #56x56\n",
        "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
        "        self.conv_23 = Mix_Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, kernel_size=[3,5,7], split_out_channels=[64,32,32] )\n",
        "\n",
        "        #28x28\n",
        "        self.conv_3 = Mix_Residual(64, num_block=9, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[96,32])\n",
        "        self.conv_34 = Mix_Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, kernel_size=[3,5,7],split_out_channels=[128,64,64] )\n",
        "\n",
        "        #14x14\n",
        "        self.conv_4 = Mix_Residual(128, num_block=16, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[192,64])\n",
        "        self.conv_5 = Mix_Depth_Wise(128, 512, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=512*2, kernel_size=[3,5,7,9],split_out_channels=[128*2,128*2,128*2,128*2] )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv0(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2_dw(out)\n",
        "        out = self.conv_23(out)\n",
        "        out = self.conv_3(out)\n",
        "        out = self.conv_34(out)\n",
        "        out = self.conv_4(out)\n",
        "        out = self.conv_5(out)\n",
        "\n",
        "        return l2_norm(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRygNex2vsR8"
      },
      "source": [
        "## VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n2r5YaLibdQ"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.convblock = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.convblock(x)\n",
        "\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG19, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            ConvBlock(3, 64),\n",
        "            ConvBlock(64, 64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            ConvBlock(64, 128),\n",
        "            ConvBlock(128, 128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            ConvBlock(128, 256),\n",
        "            ConvBlock(256, 256),\n",
        "            ConvBlock(256, 256),\n",
        "            ConvBlock(256, 256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            ConvBlock(256, 512),\n",
        "            ConvBlock(512, 512),\n",
        "            ConvBlock(512, 512),\n",
        "            ConvBlock(512, 512),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kKQ2TpxLMgG"
      },
      "source": [
        "## CCT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNvGdVRXmlnn"
      },
      "outputs": [],
      "source": [
        "class DepthWiseSeperableConv(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, bias=False):\n",
        "        super(DepthWiseSeperableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channel,\n",
        "                                   in_channel,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   groups=in_channel,\n",
        "                                   bias=bias\n",
        "                                   )\n",
        "        self.bn1 = nn.BatchNorm2d(in_channel)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pointwise = nn.Conv2d(in_channel,\n",
        "                                   out_channel,\n",
        "                                   kernel_size=1,\n",
        "                                   bias=bias\n",
        "                                   )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.pointwise(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PatchExtraction(nn.Module):\n",
        "    \"\"\" Patch extraction block:\n",
        "            - Depthwise separable convolutional layer\n",
        "            - Depthwise separable convolutional layer\n",
        "            - Pointwise convolutional layer\n",
        "\n",
        "        - MobileNet outputs feature maps from the MobileNetV1 that are padded to\n",
        "        the dimension of 16x16\n",
        "\n",
        "        - First depthwise separable convolutional layer splits into 4 patches\n",
        "        ------------------------------------------\n",
        "        Input Size:  (N, 512, 16, 16)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(PatchExtraction, self).__init__()\n",
        "        self.conv1 = DepthWiseSeperableConv(in_channel=512,\n",
        "                                            out_channel=256,\n",
        "                                            kernel_size=4,\n",
        "                                            stride=4,\n",
        "                                            padding=2)\n",
        "        self.conv2 = DepthWiseSeperableConv(in_channel=256,\n",
        "                                            out_channel=256,\n",
        "                                            kernel_size=2,\n",
        "                                            stride=2,\n",
        "                                            padding=0)\n",
        "        self.conv3 = nn.Conv2d(in_channels=256,\n",
        "                               out_channels=49,\n",
        "                               kernel_size=1,\n",
        "                               stride=1,\n",
        "                               padding=0)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvo2QXIJmiwN"
      },
      "outputs": [],
      "source": [
        "class MultiheadedSelfAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_dim,\n",
        "                 num_heads=8,\n",
        "                 attn_dropout=0.5,\n",
        "                 proj_dropout=0.5,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dim must be divisible by number of heads.\"\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_dropout = nn.Dropout(proj_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (\n",
        "            self.qkv(x) # B, N, (3*C)\n",
        "            .reshape(B, N, 3, self.num_heads, C // self.num_heads) # B, N, 3(qkv), H(eads), embed_dim\n",
        "            .permute(2, 0, 3, 1, 4) # 3, B, H(eads), N, emb_dim\n",
        "        )\n",
        "        q, k, v = torch.chunk(qkv, 3) # B, H, N, dim\n",
        "        # B,H,N,dim x B,H,dim,N -> B,H,N,N\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale # <q,k> / sqrt(d)\n",
        "        attn = attn.softmax(dim=-1) # Softmax over embedding dim\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        x = ( # B, H, N, N\n",
        "            torch.matmul(attn, v) # B,H,N,N x B,H,N,dim -> B, H, N, dim\n",
        "            .transpose(1, 2) # B, N, H, dim\n",
        "            .reshape(B, N, C) # B, N, (H*dim)\n",
        "        )\n",
        "        x = self.projection(x)\n",
        "        x = self.proj_dropout(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nh57EnYmfAZ"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_dim=192,\n",
        "                 num_heads=8,\n",
        "                 attn_dropout=0.5,\n",
        "                 proj_dropout=0.5,\n",
        "                 mlp_dropout=0.1,\n",
        "                 feedforward_dim=768,\n",
        "            ):\n",
        "        super().__init__()\n",
        "        self.norm_1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm_2 = nn.LayerNorm(embed_dim)\n",
        "        self.MHA = MultiheadedSelfAttention(embed_dim,\n",
        "                                        num_heads,\n",
        "                                        attn_dropout,\n",
        "                                        proj_dropout,\n",
        "                   )\n",
        "        self.ff = nn.Sequential(nn.Linear(embed_dim, feedforward_dim),\n",
        "                                nn.GELU(),\n",
        "                                nn.Dropout(mlp_dropout),\n",
        "                                nn.Linear(feedforward_dim, embed_dim),\n",
        "                                nn.Dropout(mlp_dropout),\n",
        "                 )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mha = self.norm_1(x)\n",
        "        mha = self.MHA(mha)\n",
        "        x = x + mha # Residual connection (Add)\n",
        "\n",
        "        x = self.norm_2(x)\n",
        "        x2 = self.ff(x)\n",
        "        x = x + x2  # Residual connection (Add)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDKYB-glmYuW"
      },
      "outputs": [],
      "source": [
        "class VCCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoders=1,\n",
        "                 num_classes=8,\n",
        "                 embed_dim=192,\n",
        "                 num_heads=8,\n",
        "                 attn_dropout=0.5,\n",
        "                 proj_dropout=0.5,\n",
        "                 mlp_dropout=0.1,\n",
        "                 feedforward_dim=768,\n",
        "            ):\n",
        "        super(VGGT, self).__init__()\n",
        "        self.vgg = MixedFeatureNet()\n",
        "        self.patcher = PatchExtraction()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(1, embed_dim)\n",
        "        self.transformer = self.create_encoders(embed_dim, num_heads,\n",
        "                                                attn_dropout, proj_dropout,\n",
        "                                                mlp_dropout, feedforward_dim,\n",
        "                                                num_encoders)\n",
        "\n",
        "        self.attention_pool = nn.Linear(embed_dim, 1)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        #for param in self.vgg.parameters():\n",
        "        #    param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "    def create_encoders(self, embed_dim=192,\n",
        "                        num_heads=8,\n",
        "                        attn_dropout=0.5,\n",
        "                        proj_dropout=0.5,\n",
        "                        mlp_dropout=0.1,\n",
        "                        feedforward_dim=768,\n",
        "                        num_layers=2,\n",
        "                       ):\n",
        "        return nn.Sequential(*[EncoderLayer(embed_dim, num_heads, attn_dropout, proj_dropout, mlp_dropout, feedforward_dim) for _ in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg(x)\n",
        "        x = self.patcher(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.fc1(x).squeeze(dim=2)\n",
        "        x = self.transformer(x)\n",
        "        x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x)\n",
        "        x = self.fc(x).squeeze(1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyucdEWVqIsl"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06zGRUxwqNO7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "batch_size = 128\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NQb06FRqOB4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_transforms = v2.Compose([\n",
        "    v2.RandAugment(num_ops=5),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = v2.Compose([\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_data = AffectNet(train_annotations, train_images, train_transforms)\n",
        "val_data = AffectNet(val_annotations, val_images, val_transforms)\n",
        "\n",
        "sample_weights = train_data.sample_weights()\n",
        "\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=batch_size,\n",
        "                          num_workers=2,\n",
        "                          sampler=WeightedRandomSampler(weights=sample_weights, num_samples=2*len(train_data), replacement=True)\n",
        "                          )\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5u0-TQoYwji",
        "outputId": "42ac1c32-25e4-4a5c-83ab-f477bad8b305"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/MCCT-6_OS_ckpt_3.pt\"\n",
        "model = VCCT(num_encoders=7)\n",
        "model.load_state_dict(torch.load(path), strict=False)\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDugNE6Kt23D"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "#optimizer.load_state_dict(torch.load(\"/content/drive/MyDrive/Graduation Project/Logs/VGGT-1/optimizer_state_dict.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I659zSt7vtBn",
        "outputId": "e3ad1a76-1367-4271-e70f-3341da65d9dd"
      },
      "outputs": [],
      "source": [
        "# Set up logging list\n",
        "logs = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    # Training phase\n",
        "    total_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "    model.train()\n",
        "    for (inputs, targets) in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False, unit=\"batch\"):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = outputs.argmax(dim=1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        total += targets.shape[0]\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_total_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for (inputs, targets) in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{epochs} - Validation', leave=False, unit=\"batch\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_total_loss += loss.item()\n",
        "            val_predicted = outputs.argmax(dim=1)\n",
        "            val_correct += (val_predicted == targets).sum().item()\n",
        "            val_total += targets.shape[0]\n",
        "\n",
        "\n",
        "    train_loss = total_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    val_loss = val_total_loss / val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    logs.append({'Epoch': epoch+1,\n",
        "                 'Loss': train_loss,\n",
        "                 'Accuracy' : train_acc,\n",
        "                 'VAL_Loss': val_loss,\n",
        "                 'VAL_Accuracy' : val_acc,\n",
        "            })\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Train loss: {train_loss:.4f} - Train acc: {train_acc:.4f} - Val loss: {val_loss:.4f} - Val acc: {val_acc:.4f}')\n",
        "\n",
        "    if (epoch + 1) % 3 == 0:\n",
        "        ckpt_path = f\"/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/MCCT-6_OS_ckpt_{epoch+1}.pt\"\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        log_df = pd.DataFrame(logs)\n",
        "        log_df.to_csv(f'/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/MCCT-6_OS_log_{epoch+1}.csv', index=False)\n",
        "        torch.save(optimizer.state_dict(), \"/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/optimizer_state_dict.pt\")\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qOvA2Q6gdA3"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPeZPOTB2GSp"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/Graduation Project/Logs/VGGT-1/VGGT-1_ckpt_77.pt\"\n",
        "\n",
        "model = VCCT()\n",
        "model.load_state_dict(torch.load(path), strict=False)\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvVXd8d22JRD"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uNVncK11wF6"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "test_transforms = v2.Compose([\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_data = AffectNet(test_annotations, test_images, test_transforms)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQK51cwK1vhT"
      },
      "outputs": [],
      "source": [
        "# Set up logging list\n",
        "test_logs = []\n",
        "\n",
        "# Test phase\n",
        "test_total_loss = 0.0\n",
        "test_correct = 0.0\n",
        "test_total = 0\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_total_loss += loss.item()\n",
        "        test_predicted = outputs.argmax(dim=1)\n",
        "        y_pred.extend(test_predicted.data.cpu().numpy())\n",
        "        y_true.extend(targets.data.cpu().numpy())\n",
        "        test_correct += (test_predicted == targets).sum().item()\n",
        "        test_total += targets.shape[0]\n",
        "\n",
        "\n",
        "test_loss = test_total_loss / test_total\n",
        "test_acc = test_correct / test_total\n",
        "\n",
        "print(f'Test loss: {test_loss:.4f} - Test acc: {test_acc:.4f}')\n",
        "\n",
        "print('Finished Evaluating')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Vq8PQledFU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "labels = [\"Neutral\", \"Happiness\", \"Sadness\", \"Surprise\", \"Fear\", \"Disgust\", \"Anger\",\n",
        "\"Contempt\"]\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cm / np.sum(cm, axis=1)[:, None], index = [i for i in labels],\n",
        "                     columns = [i for i in labels])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L7pJP0hZV3Px",
        "ZRygNex2vsR8"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4209550,
          "sourceId": 7263063,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4212265,
          "sourceId": 7266980,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30626,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
