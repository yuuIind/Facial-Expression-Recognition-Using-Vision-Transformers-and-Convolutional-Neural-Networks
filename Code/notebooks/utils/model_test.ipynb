{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdTJhB/pRXZfGhVU2Q59dI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"08BcEFApW5Bu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700411102890,"user_tz":-180,"elapsed":16391,"user":{"displayName":"MUHAMMET HAKAN TAÅžTAN","userId":"02717307810296945775"}},"outputId":"be1d1d38-2d04-4f46-f0ff-1ff2906befba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Extract the image tar files\n","!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/train_images.tar\" -C \"/\"\n","!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/val_images.tar\" -C \"/\""],"metadata":{"id":"_JV_3856XG-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define train data path\n","train_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/train_annotations.csv\"\n","train_images = \"/content/train_set/images\"\n","\n","# Define val data path\n","val_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/val_annotations.csv\"\n","val_images = \"/content/val_set/images\""],"metadata":{"id":"DcDBhiBHYe1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import dependencies\n","import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.io import read_image\n","from torchvision.transforms import Lambda\n","from torchvision.transforms import v2"],"metadata":{"id":"vInmoxp-Y311"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom Dataset Class\n","class AffectNet(Dataset):\n","    def __init__(self, annotations_file, img_root_dir, transform=None):\n","        self.annotations = pd.read_csv(annotations_file)\n","        self.root_dir = img_root_dir\n","        self.transform = transform\n","\n","        # Check if number of images and annotations match\n","        if len(self.annotations) != len(os.listdir(self.root_dir)):\n","            raise ValueError(f\"Number of images and annotations do not match:\\\n","            {len(self.annotations)} != {len(os.listdir(self.root_dir))}\"\n","                             )\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        # Get image name and create path\n","        img_name = f\"{self.annotations.iloc[idx, 0]}.jpg\"\n","        img_path = os.path.join(self.root_dir, img_name)\n","\n","        # Read image\n","        image = read_image(img_path)\n","\n","        # Get labels and convert to tensor\n","        labels = self.annotations.iloc[idx, 1:]\n","        labels = labels.to_numpy()\n","        labels = torch.from_numpy(labels.astype('float'))\n","\n","        # One-Hot Encode the categorical expressions\n","        ohe = Lambda(lambda y: torch.zeros(8, dtype=torch.float).scatter_(0, torch.tensor(int(y)), value=1))\n","        exp = ohe(labels[2])\n","        labels = torch.cat([labels[:2], exp], dim=0)\n","\n","        # Apply input transforms\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Return image and labels\n","        return image, labels"],"metadata":{"id":"LxQfVV-eaUMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","val_data = AffectNet(val_annotations, val_images)\n","val_dataloader = DataLoader(val_data, batch_size=16, shuffle=True)"],"metadata":{"id":"LgQYkL9ws9DO"},"execution_count":null,"outputs":[]}]}