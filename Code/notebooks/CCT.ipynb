{"cells":[{"cell_type":"markdown","metadata":{"id":"HeeRrAoEm8xg"},"source":["# Imports and Drive Acces"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sN3I-q84nCE2"},"outputs":[],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7579,"status":"ok","timestamp":1709214406494,"user":{"displayName":"Hakan Taştan","userId":"05753896623759361619"},"user_tz":-180},"id":"9vmCf8ql2Flz"},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torch import Tensor\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torchvision.io import read_image\n","from torchvision.transforms import v2, Lambda"]},{"cell_type":"markdown","metadata":{"id":"VeOkFS6unFCa"},"source":["# Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWk6InqPnKec"},"outputs":[],"source":["# Extract the image tar files\n","!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/train_images.tar\" -C \"/content/\"\n","!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/val_images.tar\" -C \"/content/\"\n","!tar -xf \"/content/drive/MyDrive/Graduation Project/AffectNet/test_images.tar\" -C \"/content/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoWreHZpnMoF"},"outputs":[],"source":["# Define train data path\n","train_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/train_annotations.csv\"\n","train_images = \"/content/train_images\"\n","\n","# Define test data path\n","val_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/val_annotations.csv\"\n","val_images = \"/content/val_images\"\n","\n","# Define test data path\n","test_annotations = \"/content/drive/MyDrive/Graduation Project/AffectNet/test_annotations.csv\"\n","test_images = \"/content/test_images\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dAWL-V6nOo9"},"outputs":[],"source":["class AffectNet(Dataset):\n","    \"\"\"\n","    A Dataset subclass for handling the AffectNet dataset.\n","\n","    Attributes:\n","        annotations (DataFrame): The annotations for the images.\n","        root_dir (str): The root directory where the images are stored.\n","        transform (callable, optional): Optional transform to be applied on an image.\n","    \"\"\"\n","\n","    def __init__(self, annotations_file, img_root_dir, transform=None):\n","        \"\"\"\n","        Initializes the AffectNet dataset.\n","\n","        Args:\n","            annotations_file (str): The path to the CSV file containing the annotations.\n","            img_root_dir (str): The root directory where the images are stored.\n","            transform (callable, optional): Optional transform to be applied on an image.\n","        \"\"\"\n","\n","        self.annotations = pd.read_csv(annotations_file)\n","        self.root_dir = img_root_dir\n","        self.transform = transform\n","\n","        # Check if number of images and annotations match\n","        if len(self.annotations) != len(os.listdir(self.root_dir)):\n","            raise ValueError(f\"Number of images and annotations do not match:\\\n","            {len(self.annotations)} != {len(os.listdir(self.root_dir))}\"\n","                             )\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset.\n","\n","        Returns:\n","            int: The length of the dataset.\n","        \"\"\"\n","\n","        return len(self.annotations)\n","\n","\n","    def sample_dist(self):\n","        val_count = self.annotations.expression.value_counts()\n","        val_count = val_count.to_dict()\n","        category_weights = [1 / val_count[i] for i in sorted(val_count.keys())]\n","        return category_weights\n","\n","\n","    def sample_weights(self):\n","        category_weights = self.sample_dist()\n","        sample_weights = [category_weights[exp] for exp in self.annotations.expression.values]\n","        return sample_weights\n","\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the image and its labels at the given index.\n","\n","        Args:\n","            idx (int): The index of the image.\n","\n","        Returns:\n","            tuple: A tuple containing the image, and its labels.\n","        \"\"\"\n","\n","        # Get image name and create path\n","        img_name = f\"{self.annotations.iloc[idx, 0]}.jpg\"\n","        img_path = os.path.join(self.root_dir, img_name)\n","\n","        # Read image\n","        image = read_image(img_path)\n","\n","        # Get labels and convert to tensor\n","        labels = self.annotations.iloc[idx, -1]\n","        labels = torch.tensor(labels)\n","        # Apply input transforms\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Return image and labels\n","        return image, labels"]},{"cell_type":"markdown","metadata":{"id":"0Gx0SpXXiXaR"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"4kKQ2TpxLMgG"},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709214406495,"user":{"displayName":"Hakan Taştan","userId":"05753896623759361619"},"user_tz":-180},"id":"oNvGdVRXmlnn"},"outputs":[],"source":["class Tokenizer(nn.Module):\n","    def __init__(self,\n","                 kernel_size=3, stride=2, padding=0,\n","                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n","                 n_conv_layers=2,\n","                 n_input_channels=3,\n","                 n_output_channels=64,\n","                 in_planes=64,\n","                 ):\n","        super(Tokenizer, self).__init__()\n","\n","        n_filter_list = [n_input_channels] + \\\n","                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n","                        [n_output_channels]\n","\n","        self.conv_layers = nn.Sequential(\n","            *[nn.Sequential(\n","                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n","                          kernel_size=kernel_size,\n","                          stride=stride,\n","                          padding=padding,\n","                          bias=False),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n","                             stride=pooling_stride,\n","                             padding=pooling_padding)\n","            ) for i in range(n_conv_layers) ])\n","\n","        self.flattener = nn.Flatten(2, 3)\n","\n","    def forward(self, x):\n","        return self.flattener(self.conv_layers(x)).transpose(-2, -1)"]},{"cell_type":"markdown","metadata":{"id":"QRn57g-OF9j6"},"source":["## Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":615,"status":"ok","timestamp":1709214407105,"user":{"displayName":"Hakan Taştan","userId":"05753896623759361619"},"user_tz":-180},"id":"mvo2QXIJmiwN"},"outputs":[],"source":["class MultiheadedSelfAttention(nn.Module):\n","    def __init__(self,\n","                 embed_dim,\n","                 num_heads=8,\n","                 attn_dropout=0.5,\n","                 proj_dropout=0.5,\n","                 ):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        assert embed_dim % num_heads == 0, \"Embedding dim must be divisible by number of heads.\"\n","        head_dim = embed_dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n","        self.attn_dropout = nn.Dropout(attn_dropout)\n","        self.projection = nn.Linear(embed_dim, embed_dim)\n","        self.proj_dropout = nn.Dropout(proj_dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = (\n","            self.qkv(x) # B, N, (3*C)\n","            .reshape(B, N, 3, self.num_heads, C // self.num_heads) # B, N, 3(qkv), H(eads), embed_dim\n","            .permute(2, 0, 3, 1, 4) # 3, B, H(eads), N, emb_dim\n","        )\n","        q, k, v = torch.chunk(qkv, 3) # B, H, N, dim\n","        # B,H,N,dim x B,H,dim,N -> B,H,N,N\n","        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale # <q,k> / sqrt(d)\n","        attn = attn.softmax(dim=-1) # Softmax over embedding dim\n","        attn = self.attn_dropout(attn)\n","\n","        x = ( # B, H, N, N\n","            torch.matmul(attn, v) # B,H,N,N x B,H,N,dim -> B, H, N, dim\n","            .transpose(1, 2) # B, N, H, dim\n","            .reshape(B, N, C) # B, N, (H*dim)\n","        )\n","        x = self.projection(x)\n","        x = self.proj_dropout(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1709214407106,"user":{"displayName":"Hakan Taştan","userId":"05753896623759361619"},"user_tz":-180},"id":"6nh57EnYmfAZ"},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self,\n","                 embed_dim=192,\n","                 num_heads=8,\n","                 attn_dropout=0.5,\n","                 proj_dropout=0.5,\n","                 mlp_dropout=0.1,\n","                 feedforward_dim=768,\n","            ):\n","        super().__init__()\n","        self.norm_1 = nn.LayerNorm(embed_dim)\n","        self.norm_2 = nn.LayerNorm(embed_dim)\n","        self.MHA = MultiheadedSelfAttention(embed_dim,\n","                                        num_heads,\n","                                        attn_dropout,\n","                                        proj_dropout,\n","                   )\n","        self.ff = nn.Sequential(nn.Linear(embed_dim, feedforward_dim),\n","                                nn.GELU(),\n","                                nn.Dropout(mlp_dropout),\n","                                nn.Linear(feedforward_dim, embed_dim),\n","                                nn.Dropout(mlp_dropout),\n","                 )\n","\n","    def forward(self, x):\n","        mha = self.norm_1(x)\n","        mha = self.MHA(mha)\n","        x = x + mha # Residual connection (Add)\n","\n","        x = self.norm_2(x)\n","        x2 = self.ff(x)\n","        x = x + x2  # Residual connection (Add)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"67QakzasGBGk"},"source":["## CCT"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709214407107,"user":{"displayName":"Hakan Taştan","userId":"05753896623759361619"},"user_tz":-180},"id":"HDKYB-glmYuW"},"outputs":[],"source":["class CCT(nn.Module):\n","    def __init__(self,\n","                 num_encoders=7,\n","                 num_classes=8,\n","                 embed_dim=64,\n","                 num_heads=8,\n","                 attn_dropout=0.5,\n","                 proj_dropout=0.5,\n","                 mlp_dropout=0.1,\n","                 feedforward_dim=768,\n","            ):\n","        super(CCT, self).__init__()\n","        self.tokenizer = Tokenizer(kernel_size=3, stride=2, padding=0, n_conv_layers=2)\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Linear(64, embed_dim)\n","        self.transformer = self.create_encoders(embed_dim, num_heads,\n","                                                attn_dropout, proj_dropout,\n","                                                mlp_dropout, feedforward_dim,\n","                                                num_encoders)\n","\n","        self.attention_pool = nn.Linear(embed_dim, 1)\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.fc = nn.Linear(embed_dim, num_classes)\n","\n","        #for param in self.vgg.parameters():\n","        #    param.requires_grad = False\n","\n","\n","\n","    def create_encoders(self, embed_dim=64,\n","                        num_heads=8,\n","                        attn_dropout=0.5,\n","                        proj_dropout=0.5,\n","                        mlp_dropout=0.1,\n","                        feedforward_dim=768,\n","                        num_layers=2,\n","                       ):\n","        return nn.Sequential(*[EncoderLayer(embed_dim, num_heads, attn_dropout, proj_dropout, mlp_dropout, feedforward_dim) for _ in range(num_layers)])\n","\n","\n","    def forward(self, x):\n","        x = self.tokenizer(x)\n","        x = self.fc1(x).squeeze(dim=2)\n","        x = self.transformer(x)\n","        x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze()\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"PyucdEWVqIsl"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06zGRUxwqNO7"},"outputs":[],"source":["# Hyperparameters\n","learning_rate = 0.0001\n","batch_size = 128\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","epochs = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NQb06FRqOB4"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","train_transforms = v2.Compose([\n","    v2.RandAugment(num_ops=5),\n","    v2.ToDtype(torch.float32),\n","    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transforms = v2.Compose([\n","    v2.ToDtype(torch.float32),\n","    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","train_data = AffectNet(train_annotations, train_images, train_transforms)\n","val_data = AffectNet(val_annotations, val_images, val_transforms)\n","\n","sample_weights = train_data.sample_weights()\n","\n","train_loader = DataLoader(train_data,\n","                          batch_size=batch_size,\n","                          num_workers=2,\n","                          sampler=WeightedRandomSampler(weights=sample_weights, num_samples=3*len(train_data), replacement=True)\n","                          )\n","val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1709214510352,"user":{"displayName":"Hakan Taştan","userId":"05753896623759361619"},"user_tz":-180},"id":"t5u0-TQoYwji","outputId":"984420bd-c9ac-43a5-b029-247603f6b786"},"outputs":[],"source":["#path = \"/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/MCCT-6_OS_ckpt_3.pt\"\n","model = CCT(num_encoders=7)\n","#model.load_state_dict(torch.load(path), strict=False)\n","#model.to(device)\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDugNE6Kt23D"},"outputs":[],"source":["# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","#optimizer.load_state_dict(torch.load(\"/content/drive/MyDrive/Graduation Project/Logs/VGGT-1/optimizer_state_dict.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I659zSt7vtBn","outputId":"e3ad1a76-1367-4271-e70f-3341da65d9dd"},"outputs":[],"source":["# Set up logging list\n","logs = []\n","\n","for epoch in range(3, epochs):\n","    # Training phase\n","    total_loss = 0.0\n","    correct = 0.0\n","    total = 0\n","    model.train()\n","    for (inputs, targets) in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False, unit=\"batch\"):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, targets)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        predicted = outputs.argmax(dim=1)\n","        correct += (predicted == targets).sum().item()\n","        total += targets.shape[0]\n","\n","    # Validation phase\n","    model.eval()\n","    val_total_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            val_total_loss += loss.item()\n","            val_predicted = outputs.argmax(dim=1)\n","            val_correct += (val_predicted == targets).sum().item()\n","            val_total += targets.shape[0]\n","\n","\n","    train_loss = total_loss / total\n","    train_acc = correct / total\n","\n","    val_loss = val_total_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    logs.append({'Epoch': epoch+1,\n","                 'Loss': train_loss,\n","                 'Accuracy' : train_acc,\n","                 'VAL_Loss': val_loss,\n","                 'VAL_Accuracy' : val_acc,\n","            })\n","\n","    print(f'Epoch {epoch + 1}/{epochs} - Train loss: {train_loss:.4f} - Train acc: {train_acc:.4f} - Val loss: {val_loss:.4f} - Val acc: {val_acc:.4f}')\n","\n","\n","    ckpt_path = f\"/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/MCCT-6_OS_ckpt_{epoch+1}.pt\"\n","    torch.save(model.state_dict(), ckpt_path)\n","    log_df = pd.DataFrame(logs)\n","    log_df.to_csv(f'/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/MCCT-6_OS_log_{epoch+1}.csv', index=False)\n","    torch.save(optimizer.state_dict(), \"/content/drive/MyDrive/Graduation Project/Logs/MCCT-6_OS/optimizer_state_dict.pt\")\n","\n","\n","print('Finished Training')"]},{"cell_type":"markdown","metadata":{"id":"2qOvA2Q6gdA3"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPeZPOTB2GSp"},"outputs":[],"source":["path = \"/content/drive/MyDrive/Graduation Project/Logs/VGGT-1/VGGT-1_ckpt_77.pt\"\n","model = CCT()\n","model.load_state_dict(torch.load(path), strict=False)\n","model.to(device)\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvVXd8d22JRD"},"outputs":[],"source":["# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uNVncK11wF6"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","test_transforms = v2.Compose([\n","    v2.ToDtype(torch.float32),\n","    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","test_data = AffectNet(test_annotations, test_images, test_transforms)\n","\n","test_loader = DataLoader(test_data,\n","                          batch_size=32,\n","                          shuffle=True\n","                          )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQK51cwK1vhT"},"outputs":[],"source":["# Set up logging list\n","test_logs = []\n","\n","# Test phase\n","test_total_loss = 0.0\n","test_correct = 0.0\n","test_total = 0\n","model.eval()\n","y_pred = []\n","y_true = []\n","with torch.no_grad():\n","    for inputs, targets in test_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        test_total_loss += loss.item()\n","        test_predicted = outputs.argmax(dim=1)\n","        y_pred.extend(test_predicted.data.cpu().numpy())\n","        y_true.extend(targets.data.cpu().numpy())\n","        test_correct += (test_predicted == targets).sum().item()\n","        test_total += targets.shape[0]\n","\n","\n","test_loss = test_total_loss / test_total\n","test_acc = test_correct / test_total\n","\n","print(f'Test loss: {test_loss:.4f} - Test acc: {test_acc:.4f}')\n","\n","print('Finished Evaluating')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7Vq8PQledFU"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","\n","labels = [\"Neutral\", \"Happiness\", \"Sadness\", \"Surprise\", \"Fear\", \"Disgust\", \"Anger\",\n","\"Contempt\"]\n","cm = confusion_matrix(y_true, y_pred)\n","df_cm = pd.DataFrame(cm / np.sum(cm, axis=1)[:, None], index = [i for i in labels],\n","                     columns = [i for i in labels])\n","plt.figure(figsize = (12,7))\n","sn.heatmap(df_cm, annot=True)\n","plt.savefig('output.png')"]}],"metadata":{"colab":{"collapsed_sections":["VeOkFS6unFCa","4kKQ2TpxLMgG","QRn57g-OF9j6"],"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4209550,"sourceId":7263063,"sourceType":"datasetVersion"},{"datasetId":4212265,"sourceId":7266980,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
